{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import KinQueryDataset, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MARK_PAD = \"<PAD>\"\n",
    "MARK_UNK = \"<UNK>\"\n",
    "\n",
    "MARKS = [MARK_PAD, MARK_UNK]\n",
    "ID_PAD = 0\n",
    "ID_UNK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dict(dict_path, max_vocab=None):\n",
    "    logging.info(\"Try load dict from {}.\".format(dict_path))\n",
    "    try:\n",
    "        dict_file = open(dict_path)\n",
    "        dict_data = dict_file.readlines()\n",
    "        dict_file.close()\n",
    "    except:\n",
    "        logging.info(\n",
    "            \"Load dict {dict} failed, create later.\".format(dict=dict_path))\n",
    "        return None\n",
    "\n",
    "    dict_data = list(map(lambda x: x.split(), dict_data))\n",
    "    if max_vocab:\n",
    "        dict_data = list(filter(lambda x: int(x[0]) < max_vocab, dict_data))\n",
    "    tok2id = dict(map(lambda x: (x[1], int(x[0])), dict_data))\n",
    "    id2tok = dict(map(lambda x: (int(x[0]), x[1]), dict_data))\n",
    "    logging.info(\n",
    "        \"Load dict {} with {} words.\".format(dict_path, len(tok2id)))\n",
    "    return (tok2id, id2tok)\n",
    "\n",
    "def create_dict(dict_path, corpus, max_vocab=None):\n",
    "    logging.info(\"Create dict {}.\".format(dict_path))\n",
    "    counter = {}\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            try:\n",
    "                counter[word] += 1\n",
    "            except:\n",
    "                counter[word] = 1\n",
    "\n",
    "    for mark_t in MARKS:\n",
    "        if mark_t in counter:\n",
    "            del counter[mark_t]\n",
    "            logging.warning(\"{} appears in corpus.\".format(mark_t))\n",
    "\n",
    "    counter = list(counter.items())\n",
    "    counter.sort(key=lambda x: -x[1])\n",
    "    words = list(map(lambda x: x[0], counter))\n",
    "    words = [MARK_PAD, MARK_UNK] + words\n",
    "    if max_vocab:\n",
    "        words = words[:max_vocab]\n",
    "\n",
    "    tok2id = dict()\n",
    "    id2tok = dict()\n",
    "    with open(dict_path, 'w') as dict_file:\n",
    "        for idx, tok in enumerate(words):\n",
    "            print(idx, tok, file=dict_file)\n",
    "            tok2id[tok] = idx\n",
    "            id2tok[idx] = tok\n",
    "\n",
    "    logging.info(\n",
    "        \"Create dict {} with {} words.\".format(dict_path, len(words)))\n",
    "    return (tok2id, id2tok)\n",
    "\n",
    "def corpus_map2id(data, tok2id):\n",
    "    ret = []\n",
    "    unk = 0\n",
    "    tot = 0\n",
    "    for doc in data:\n",
    "        tmp = []\n",
    "        for word in doc:\n",
    "            tot += 1\n",
    "            try:\n",
    "                tmp.append(tok2id[word])\n",
    "            except:\n",
    "                tmp.append(ID_UNK)\n",
    "                unk += 1\n",
    "        ret.append(tmp)\n",
    "    return ret, (tot - unk) / tot\n",
    "\n",
    "\n",
    "def sen_map2tok(sen, id2tok):\n",
    "    return list(map(lambda x: id2tok[x], sen))\n",
    "\n",
    "\n",
    "def load_data(data,\n",
    "              doc_dict_path,\n",
    "              max_doc_vocab=None):\n",
    "    with open(data,'rt', encoding=\"utf-8\") as docfile:\n",
    "        docs = docfile.readlines()\n",
    "\n",
    "    docs = list(map(lambda x: x.split(), data))\n",
    "\n",
    "    doc_dict = load_dict(doc_dict_path, max_doc_vocab)\n",
    "    if doc_dict is None:\n",
    "        doc_dict = create_dict(doc_dict_path, docs, max_doc_vocab)\n",
    "\n",
    "    docid, cover = corpus_map2id(docs, doc_dict[0])\n",
    "    print(\"Doc dict covers %.2f%% words.\"%(cover*100))\n",
    "\n",
    "    return docid, doc_dict\n",
    "\n",
    "def load_test_data(doc, doc_dict):\n",
    "    docs = corpus_preprocess(doc)\n",
    "\n",
    "    print(\"Load %d testing documents.\"%(len(docs)))\n",
    "    docs = list(map(lambda x: x.split(), docs))\n",
    "\n",
    "    docid, cover = corpus_map2id(docs, doc_dict[0])\n",
    "    print(\"Doc dict covers %.2f words.\"%(cover*100))\n",
    "\n",
    "    return docid\n",
    "\n",
    "def corpus_preprocess(corpus):\n",
    "    import re\n",
    "    ret = []\n",
    "    for line in corpus:\n",
    "        x = re.sub('\\\\d', '#', line)\n",
    "        ret.append(x)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sen_postprocess(sen):\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KinQueryDataset:\n",
    "    def __init__(self, dataset_path: str, max_length: int, vocab_size: int):\n",
    "        # 데이터, 레이블 각각의 경로\n",
    "        queries_path = os.path.join(dataset_path, 'train', 'train_data')\n",
    "        labels_path = os.path.join(dataset_path, 'train', 'train_label')\n",
    "\n",
    "        # 지식인 데이터를 읽고 preprocess까지 진행합니다\n",
    "        dict_name = str(vocab_size)+\"dict\"\n",
    "        dicId , dic = load_data(queries_path, dict_name , vocab_size)\n",
    "        self.queries = preprocess(dicId, dic, max_length)\n",
    "        # 지식인 레이블을 읽고 preprocess까지 진행합니다.\n",
    "        \n",
    "        with open(labels_path) as f:\n",
    "            self.labels = np.array([[np.float32(x)] for x in f.readlines()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.queries[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data: list, dic: list, max_length: int):\n",
    "    vectorized_data = data\n",
    "    zero_padding = np.zeros((len(data), max_length), dtype=np.int32)\n",
    "    #print(data)\n",
    "    for idx, seq in enumerate(vectorized_data):\n",
    "        length = len(seq)\n",
    "        #print(idx, seq)\n",
    "        if length >= max_length:\n",
    "            length = max_length\n",
    "            zero_padding[idx, :length] = np.array(seq)[:length]\n",
    "        elif(length == 0):\n",
    "            zero_padding[idx,] = zero_padding[idx,]\n",
    "        else:\n",
    "            zero_padding[idx,] = np.append(zero_padding[idx,:-length], np.array(seq))\n",
    "    return zero_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATASET_PATH = '../sample_data/movie_review/'\n",
    "#queries_path = os.path.join(DATASET_PATH, 'train', 'train_data')\n",
    "#with open(queries_path, 'rt', encoding='utf8') as f:\n",
    "#            print(f.readlines())\n",
    "#dataset = KinQueryDataset(DATASET_PATH, strmaxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_one_hot(labels):\n",
    "    one_hot = (np.arange(output_size) == labels[:]).astype(np.int32)\n",
    "    return one_hot\n",
    "def catFromOut(output):\n",
    "    idx = []\n",
    "    for i in output:\n",
    "        ids = np.argmax(i, axis = 0)\n",
    "        idx.append(ids)\n",
    "    idx = np.array(idx)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shaep (?, 200)\n",
      "output shape (?, 1)\n",
      "lables shaep (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# User options\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "embedding = 30\n",
    "strmaxlen = 50\n",
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "# 모델의 specification\n",
    "input_size = embedding*strmaxlen\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "learning_rate = 0.01\n",
    "dict_size = 424\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, strmaxlen])\n",
    "y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "# 임베딩\n",
    "char_embedding = tf.get_variable('char_embedding', [dict_size, embedding])\n",
    "embedded = tf.nn.embedding_lookup(char_embedding, x)\n",
    "\n",
    "# 첫 번째 레이어\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.9)\n",
    "#cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "\n",
    "#multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell, cell2])\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "output = output[-1]\n",
    "print(\"output shaep\",output.shape)\n",
    "\n",
    "# 두 번째 (아웃풋) 레이어\n",
    "second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "second_layer_bias = bias_variable([output_size])\n",
    "output = tf.matmul(output, second_layer_weight) + second_layer_bias\n",
    "#output_sigmoid = tf.sigmoid(output)\n",
    "print(\"output shape\",output.shape)\n",
    "print(\"lables shaep\",y_.shape)\n",
    "# loss와 optimizer\n",
    "#global_step = tf.Variable(0)\n",
    "foutput = tf.nn.log_softmax(output)\n",
    "cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=output))\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_))\n",
    "#learning_rate= tf.train.exponential_decay(learning_rate, global_step, 10000, 0.75)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6ebab85488c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#epochs = 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKinQueryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdataset_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mone_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_len\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-302c4cdc8f36>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path, max_length, vocab_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# 지식인 데이터를 읽고 preprocess까지 진행합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdict_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"dict\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdicId\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_name\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 지식인 레이블을 읽고 preprocess까지 진행합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_util' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "all_losses=[]\n",
    "current_loss=0\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "#epochs = 10\n",
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen, 3000)\n",
    "dataset_len = len(dataset)\n",
    "one_batch_size = dataset_len//batch\n",
    "if dataset_len % batch != 0:\n",
    "    one_batch_size += 1\n",
    "# epoch마다 학습을 수행합니다.\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "            avg_loss = 0.0\n",
    "            for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "                _, loss = sess.run([train_step, cost],\n",
    "                                   feed_dict={x: data, y_: labels})\n",
    "                print('Batch : ', i + 1, '/', one_batch_size,\n",
    "                      ', MSE in this minibatch: ', float(loss))\n",
    "                avg_loss += float(loss)\n",
    "            print('epoch:', epoch, ' train_loss:', float(avg_loss/one_batch_size),timeSince(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
