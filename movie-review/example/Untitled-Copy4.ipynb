{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import KinQueryDataset, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dict(dict_path, max_vocab=None):\n",
    "    logging.info(\"Try load dict from {}.\".format(dict_path))\n",
    "    try:\n",
    "        dict_file = open(dict_path)\n",
    "        dict_data = dict_file.readlines()\n",
    "        dict_file.close()\n",
    "    except:\n",
    "        logging.info(\n",
    "            \"Load dict {dict} failed, create later.\".format(dict=dict_path))\n",
    "        return None\n",
    "\n",
    "    dict_data = list(map(lambda x: x.split(), dict_data))\n",
    "    if max_vocab:\n",
    "        dict_data = list(filter(lambda x: int(x[0]) < max_vocab, dict_data))\n",
    "    tok2id = dict(map(lambda x: (x[1], int(x[0])), dict_data))\n",
    "    id2tok = dict(map(lambda x: (int(x[0]), x[1]), dict_data))\n",
    "    logging.info(\n",
    "        \"Load dict {} with {} words.\".format(dict_path, len(tok2id)))\n",
    "    return (tok2id, id2tok)\n",
    "\n",
    "def create_dict(dict_path, corpus, max_vocab=None):\n",
    "    logging.info(\"Create dict {}.\".format(dict_path))\n",
    "    counter = {}\n",
    "    for line in corpus:\n",
    "        for word in line:\n",
    "            try:\n",
    "                counter[word] += 1\n",
    "            except:\n",
    "                counter[word] = 1\n",
    "\n",
    "    for mark_t in MARKS:\n",
    "        if mark_t in counter:\n",
    "            del counter[mark_t]\n",
    "            logging.warning(\"{} appears in corpus.\".format(mark_t))\n",
    "\n",
    "    counter = list(counter.items())\n",
    "    counter.sort(key=lambda x: -x[1])\n",
    "    words = list(map(lambda x: x[0], counter))\n",
    "    words = [MARK_PAD, MARK_UNK] + words\n",
    "    if max_vocab:\n",
    "        words = words[:max_vocab]\n",
    "\n",
    "    tok2id = dict()\n",
    "    id2tok = dict()\n",
    "    with open(dict_path, 'w') as dict_file:\n",
    "        for idx, tok in enumerate(words):\n",
    "            print(idx, tok, file=dict_file)\n",
    "            tok2id[tok] = idx\n",
    "            id2tok[idx] = tok\n",
    "\n",
    "    logging.info(\n",
    "        \"Create dict {} with {} words.\".format(dict_path, len(words)))\n",
    "    return (tok2id, id2tok)\n",
    "\n",
    "def corpus_map2id(data, tok2id):\n",
    "    ret = []\n",
    "    unk = 0\n",
    "    tot = 0\n",
    "    for doc in data:\n",
    "        tmp = []\n",
    "        for word in doc:\n",
    "            tot += 1\n",
    "            try:\n",
    "                tmp.append(tok2id[word])\n",
    "            except:\n",
    "                tmp.append(ID_UNK)\n",
    "                unk += 1\n",
    "        ret.append(tmp)\n",
    "    return ret, (tot - unk) / tot\n",
    "\n",
    "\n",
    "def sen_map2tok(sen, id2tok):\n",
    "    return list(map(lambda x: id2tok[x], sen))\n",
    "\n",
    "\n",
    "def load_data(doc_filename,\n",
    "              doc_dict_path,\n",
    "              max_doc_vocab=None):\n",
    "    logging.info(\n",
    "        \"Load document from {}.\".format(\n",
    "            doc_filename))\n",
    "\n",
    "    with open(doc_filename) as docfile:\n",
    "        docs = docfile.readlines()\n",
    "\n",
    "    docs = list(map(lambda x: x.split(), docs))\n",
    "\n",
    "    doc_dict = load_dict(doc_dict_path, max_doc_vocab)\n",
    "    if doc_dict is None:\n",
    "        doc_dict = create_dict(doc_dict_path, docs, max_doc_vocab)\n",
    "\n",
    "    docid, cover = corpus_map2id(docs, doc_dict[0])\n",
    "    logging.info(\n",
    "        \"Doc dict covers {:.2f}% words.\".format(cover * 100))\n",
    "\n",
    "    return docid, doc_dict\n",
    "\n",
    "def corpus_preprocess(corpus):\n",
    "    import re\n",
    "    ret = []\n",
    "    for line in corpus:\n",
    "        x = re.sub('\\\\d', '#', line)\n",
    "        ret.append(x)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sen_postprocess(sen):\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KinQueryDataset:\n",
    "    def __init__(self, dataset_path: str, max_length: int):\n",
    "        # 데이터, 레이블 각각의 경로\n",
    "        \n",
    "        queries_path = os.path.join(dataset_path, 'train', 'train_data')\n",
    "        labels_path = os.path.join(dataset_path, 'train', 'train_label')\n",
    "        # 지식인 데이터를 읽고 preprocess까지 진행합니다\n",
    "        dicId , dic = load_data(queries_path, \"doc_dict\", 3000)\n",
    "        self.queries = preprocess(dicId, max_length)\n",
    "        # 지식인 레이블을 읽고 preprocess까지 진행합니다.\n",
    "        with open(labels_path) as f:\n",
    "            self.labels = np.array([[np.float32(x)] for x in f.readlines()])\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.queries[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data: list, max_length: int):\n",
    "    vectorized_data = data\n",
    "    zero_padding = np.zeros((len(data), max_length), dtype=np.int32)\n",
    "    for idx, seq in enumerate(vectorized_data):\n",
    "        length = len(seq)\n",
    "        if length >= max_length:\n",
    "            length = max_length\n",
    "            zero_padding[idx, :length] = np.array(seq)[:length]\n",
    "        else:\n",
    "            zero_padding[idx,] = np.append(zero_padding[idx,:-length], np.array(seq))\n",
    "    return zero_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shaep (?, 200)\n",
      "output shaep (?, 1)\n",
      "lables shaep (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# User options\n",
    "batch = 200\n",
    "epochs = 100\n",
    "\n",
    "embedding = 10\n",
    "strmaxlen = 50\n",
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "# 모델의 specification\n",
    "input_size = embedding*strmaxlen\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "learning_rate = 0.001\n",
    "character_size = 424\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, strmaxlen])\n",
    "y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "# 임베딩\n",
    "char_embedding = tf.get_variable('char_embedding', [character_size, embedding])\n",
    "embedded = tf.nn.embedding_lookup(char_embedding, x)\n",
    "\n",
    "# 첫 번째 레이어\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.9)\n",
    "#cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "\n",
    "#multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell, cell2])\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "output = output[-1]\n",
    "print(\"output shaep\",output.shape)\n",
    "\n",
    "# 두 번째 (아웃풋) 레이어\n",
    "second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "second_layer_bias = bias_variable([output_size])\n",
    "foutput = tf.matmul(output, second_layer_weight) + second_layer_bias\n",
    "#output_sigmoid = tf.sigmoid(output)\n",
    "print(\"output shaep\",foutput.shape)\n",
    "print(\"lables shaep\",y_.shape)\n",
    "# loss와 optimizer\n",
    "#global_step = tf.Variable(0)\n",
    "\n",
    "cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=foutput))\n",
    "#cost = tf.reduce_mean(-(y_ * tf.log(output)) - (1-y_) * tf.log(1-output))\n",
    "#learning_rate= tf.train.exponential_decay(learning_rate, global_step, 10000, 0.75)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "#train_step = tf.train.AdadeltaOptimizer(1.0, 0.95, 1e-6).minimize(cost)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 14.000910227328417 0m 1s\n",
      "epoch: 1  train_loss: 7.394086530561304 0m 3s\n",
      "epoch: 2  train_loss: 7.2915475905843845 0m 5s\n",
      "epoch: 3  train_loss: 7.275028882750716 0m 7s\n",
      "epoch: 4  train_loss: 7.255075617391141 0m 9s\n",
      "epoch: 5  train_loss: 7.2094476247123644 0m 11s\n",
      "epoch: 6  train_loss: 7.12834645439566 0m 12s\n",
      "epoch: 7  train_loss: 6.963680754406987 0m 14s\n",
      "epoch: 8  train_loss: 6.4129705685870855 0m 16s\n",
      "epoch: 9  train_loss: 4.052321985119034 0m 18s\n",
      "epoch: 10  train_loss: 2.4801265441935874 0m 20s\n",
      "epoch: 11  train_loss: 1.6705004655090068 0m 22s\n",
      "epoch: 12  train_loss: 1.2692343617034516 0m 23s\n",
      "epoch: 13  train_loss: 0.8191741406489175 0m 25s\n",
      "epoch: 14  train_loss: 0.57157738192323 0m 27s\n",
      "epoch: 15  train_loss: 0.3581218612378966 0m 29s\n",
      "epoch: 16  train_loss: 0.2804601522301004 0m 31s\n",
      "epoch: 17  train_loss: 0.2933378097546223 0m 32s\n",
      "epoch: 18  train_loss: 0.2424758811995527 0m 34s\n",
      "epoch: 19  train_loss: 0.21722462545877985 0m 36s\n",
      "epoch: 20  train_loss: 0.17963464545993413 0m 38s\n",
      "epoch: 21  train_loss: 0.19423980562382512 0m 40s\n",
      "epoch: 22  train_loss: 0.18967162576159055 0m 41s\n",
      "epoch: 23  train_loss: 0.13380010394462047 0m 43s\n",
      "epoch: 24  train_loss: 0.1399462490427166 0m 45s\n",
      "epoch: 25  train_loss: 0.11865922299547721 0m 47s\n",
      "epoch: 26  train_loss: 0.15080629347323898 0m 49s\n",
      "epoch: 27  train_loss: 0.20575961058233946 0m 51s\n",
      "epoch: 28  train_loss: 0.11150591208155607 0m 52s\n",
      "epoch: 29  train_loss: 0.13416835030018723 0m 54s\n",
      "epoch: 30  train_loss: 0.11124815327559326 0m 56s\n",
      "epoch: 31  train_loss: 0.09576885939556726 0m 58s\n",
      "epoch: 32  train_loss: 0.15733131833466335 1m 0s\n",
      "epoch: 33  train_loss: 0.11215098693812132 1m 1s\n",
      "epoch: 34  train_loss: 0.10266626675387304 1m 3s\n",
      "epoch: 35  train_loss: 0.09226530408063256 1m 5s\n",
      "epoch: 36  train_loss: 0.07682642633837837 1m 7s\n",
      "epoch: 37  train_loss: 0.07589503752978574 1m 9s\n",
      "epoch: 38  train_loss: 0.10363012822930376 1m 11s\n",
      "epoch: 39  train_loss: 0.07744168149919264 1m 12s\n",
      "epoch: 40  train_loss: 0.08670892735718154 1m 14s\n",
      "epoch: 41  train_loss: 0.09231875160163161 1m 16s\n",
      "epoch: 42  train_loss: 0.06950979234727264 1m 18s\n",
      "epoch: 43  train_loss: 0.09053848500078747 1m 20s\n",
      "epoch: 44  train_loss: 0.09400203960691877 1m 21s\n",
      "epoch: 45  train_loss: 0.06083001870869101 1m 23s\n",
      "epoch: 46  train_loss: 0.07893049440136675 1m 25s\n",
      "epoch: 47  train_loss: 0.09079099471783364 1m 27s\n",
      "epoch: 48  train_loss: 0.05674420219831994 1m 29s\n",
      "epoch: 49  train_loss: 0.08942258588922414 1m 30s\n",
      "epoch: 50  train_loss: 0.11033964227983879 1m 32s\n",
      "epoch: 51  train_loss: 0.09909772289705802 1m 34s\n",
      "epoch: 52  train_loss: 0.12253097552670242 1m 36s\n",
      "epoch: 53  train_loss: 0.09496940208209367 1m 38s\n",
      "epoch: 54  train_loss: 0.10211701310135285 1m 40s\n",
      "epoch: 55  train_loss: 0.07380876023345723 1m 41s\n",
      "epoch: 56  train_loss: 0.13211308350723938 1m 43s\n",
      "epoch: 57  train_loss: 0.11196141241647654 1m 45s\n",
      "epoch: 58  train_loss: 0.08983720008694877 1m 47s\n",
      "epoch: 59  train_loss: 0.1467802875666285 1m 49s\n",
      "epoch: 60  train_loss: 0.11856412881708277 1m 50s\n",
      "epoch: 61  train_loss: 0.08938120388333645 1m 52s\n",
      "epoch: 62  train_loss: 0.09001281796114097 1m 54s\n",
      "epoch: 63  train_loss: 0.06223290774152838 1m 56s\n",
      "epoch: 64  train_loss: 0.04964254385069121 1m 58s\n",
      "epoch: 65  train_loss: 0.054135246188426744 1m 59s\n",
      "epoch: 66  train_loss: 0.12549108898645364 2m 1s\n",
      "epoch: 67  train_loss: 0.10606635203633988 2m 3s\n",
      "epoch: 68  train_loss: 0.11256131055273659 2m 5s\n",
      "epoch: 69  train_loss: 0.11958654397045036 2m 7s\n",
      "epoch: 70  train_loss: 0.09741720680246213 2m 8s\n",
      "epoch: 71  train_loss: 0.06381872307561748 2m 10s\n",
      "epoch: 72  train_loss: 0.037543101163687725 2m 12s\n",
      "epoch: 73  train_loss: 0.04510679183587395 2m 14s\n",
      "epoch: 74  train_loss: 0.04388784232407284 2m 16s\n",
      "epoch: 75  train_loss: 0.06540011100830874 2m 17s\n",
      "epoch: 76  train_loss: 0.09458317763246414 2m 19s\n",
      "epoch: 77  train_loss: 0.06159043557146726 2m 21s\n",
      "epoch: 78  train_loss: 0.08966826923770156 2m 23s\n",
      "epoch: 79  train_loss: 0.14602063110943103 2m 25s\n",
      "epoch: 80  train_loss: 0.06593167063846667 2m 26s\n",
      "epoch: 81  train_loss: 0.07135433616403873 2m 28s\n",
      "epoch: 82  train_loss: 0.05186066348929994 2m 30s\n",
      "epoch: 83  train_loss: 0.07577195961638498 2m 32s\n",
      "epoch: 84  train_loss: 0.03704255479485859 2m 34s\n",
      "epoch: 85  train_loss: 0.06929892399840976 2m 35s\n",
      "epoch: 86  train_loss: 0.09573022991974531 2m 37s\n",
      "epoch: 87  train_loss: 0.1830281872781368 2m 39s\n",
      "epoch: 88  train_loss: 0.07676323855460643 2m 41s\n",
      "epoch: 89  train_loss: 0.08081786478561791 2m 42s\n",
      "epoch: 90  train_loss: 0.03613780186026467 2m 44s\n",
      "epoch: 91  train_loss: 0.04150265431831371 2m 46s\n",
      "epoch: 92  train_loss: 0.04581101465422949 2m 48s\n",
      "epoch: 93  train_loss: 0.04579379763623008 2m 50s\n",
      "epoch: 94  train_loss: 0.062401979935831686 2m 52s\n",
      "epoch: 95  train_loss: 0.09494763435909123 2m 54s\n",
      "epoch: 96  train_loss: 0.07459736057701087 2m 55s\n",
      "epoch: 97  train_loss: 0.10520145842522899 2m 57s\n",
      "epoch: 98  train_loss: 0.10808180978044483 2m 59s\n",
      "epoch: 99  train_loss: 0.10885836833995624 3m 1s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "batch = 1\n",
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)\n",
    "one_batch_size = dataset_len//batch\n",
    "if dataset_len % batch != 0:\n",
    "    one_batch_size += 1\n",
    "# epoch마다 학습을 수행합니다.\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "        #zero = np.zeros([batch,1])\n",
    "        #zero[0] = \n",
    "        _, loss, o = sess.run([train_step, cost, foutput],\n",
    "                           feed_dict={x: data, y_: labels})\n",
    "        #print('Batch : ', i + 1, '/', one_batch_size,', BCE in this minibatch: ', (loss))\n",
    "        avg_loss += float(loss)\n",
    "        #print(\"output\",o.reshape((1,-1)))\n",
    "        #print(\"lablels\", labels.reshape((1,-1)))\n",
    "    print('epoch:', epoch, ' train_loss:', float(avg_loss/one_batch_size),timeSince(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output [[ 9.63156509]]\n",
      "lablels [[ 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"output\",o.reshape((1,-1)))\n",
    "print(\"lablels\", labels.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_util\n",
    "def infer(raw_data, **kwargs):\n",
    "        # dataset.py에서 작성한 preprocess 함수를 호출하여, 문자열을 벡터로 변환합니다\n",
    "        preprocessed_data, dod = data_util.load_data(raw_data,'doc_dict', strmaxlen)\n",
    "        queries = preprocess(preprocessed_data, strmaxlen)\n",
    "        # 저장한 모델에 입력값을 넣고 prediction 결과를 리턴받습니다\n",
    "        #print(preprocessed_data)\n",
    "        pred = sess.run(foutput, feed_dict={x: queries})\n",
    "\n",
    "        if np.any((np.logical_or(pred<0, pred>10))):\n",
    "            pred[np.where(pred<0)] = 0\n",
    "            pred[np.where(pred>10)] = 10\n",
    "        point = pred.flatten()\n",
    "        # DONOTCHANGE: They are reserved for nsml\n",
    "        # 리턴 결과는 [(확률, 0 or 1)] 의 형태로 보내야만 리더보드에 올릴 수 있습니다. 리더보드 결과에 확률의 값은 영향을 미치지 않습니다\n",
    "        return list(zip(np.zeros(len(point)), point))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 6.7659721), (0.0, 6.8927441), (0.0, 8.9276371), (0.0, 6.8059282), (0.0, 9.0036631), (0.0, 10.0), (0.0, 6.3613091), (0.0, 10.0), (0.0, 6.2177), (0.0, 7.7517004), (0.0, 9.5121126), (0.0, 6.1602783), (0.0, 9.2664976), (0.0, 8.8918076), (0.0, 9.0356712), (0.0, 8.9323883), (0.0, 9.4920063), (0.0, 8.9323883), (0.0, 8.6010523), (0.0, 9.4178371), (0.0, 6.941514), (0.0, 7.1960545), (0.0, 6.9128475), (0.0, 7.7637119), (0.0, 6.8020983), (0.0, 8.300272), (0.0, 9.2337198), (0.0, 9.2530594), (0.0, 6.5494986), (0.0, 6.5494986), (0.0, 8.9254465), (0.0, 9.2337198), (0.0, 7.668149), (0.0, 8.3516264), (0.0, 6.5494986), (0.0, 4.5342312), (0.0, 6.5494986), (0.0, 6.6981001), (0.0, 5.1153784), (0.0, 2.7503002), (0.0, 7.1914301), (0.0, 8.7108364), (0.0, 8.9117308), (0.0, 1.9282413), (0.0, 7.1914301), (0.0, 7.0425749), (0.0, 7.4378223), (0.0, 6.5494986), (0.0, 6.8590784), (0.0, 7.1914301), (0.0, 6.8583755), (0.0, 9.720521), (0.0, 4.2906008), (0.0, 9.6816006), (0.0, 9.0883255), (0.0, 6.5494986), (0.0, 6.5494986), (0.0, 6.7665701), (0.0, 7.1914301), (0.0, 7.1914301), (0.0, 6.5494986), (0.0, 8.0482693), (0.0, 7.8061161), (0.0, 6.9128475), (0.0, 7.9966564), (0.0, 4.0362606), (0.0, 6.3367367), (0.0, 7.1914301), (0.0, 7.6610956), (0.0, 8.6396561), (0.0, 7.6554089), (0.0, 6.8059282), (0.0, 8.6396561), (0.0, 6.5494986), (0.0, 6.5494986), (0.0, 7.1914301), (0.0, 3.5736988), (0.0, 8.1164198), (0.0, 3.5736988), (0.0, 1.1277606), (0.0, 6.5494986), (0.0, 1.2471827), (0.0, 2.5017641), (0.0, 6.4522219), (0.0, 6.5494986), (0.0, 6.9128475), (0.0, 6.5494986), (0.0, 6.9128475), (0.0, 6.5494986), (0.0, 6.655654), (0.0, 6.6098108), (0.0, 8.5062075), (0.0, 6.9128475), (0.0, 6.9128475), (0.0, 6.9128475), (0.0, 6.9128475), (0.0, 9.0003576), (0.0, 6.5494986), (0.0, 6.9128475), (0.0, 7.1914301), (0.0, 6.9128475), (0.0, 6.8059282), (0.0, 6.9128475), (0.0, 6.5494986), (0.0, 6.9128475), (0.0, 6.837266), (0.0, 6.5494986)]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATASET_PATH, 'train/train_data'), 'rt', encoding='utf-8') as f:\n",
    "    queries = f.readlines()\n",
    "res = []\n",
    "for batch in _batch_loader(queries, 200):\n",
    "    temp_res = infer(batch)\n",
    "    res += temp_res\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
