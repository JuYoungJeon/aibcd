{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import KinQueryDataset, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shaep (?, 200)\n",
      "output shaep (?, 1)\n",
      "lables shaep (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# User options\n",
    "batch = 100\n",
    "epochs = 100\n",
    "\n",
    "embedding = 30\n",
    "strmaxlen = 200\n",
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "# 모델의 specification\n",
    "input_size = embedding*strmaxlen\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "learning_rate = 0.001\n",
    "character_size = 251\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, strmaxlen])\n",
    "y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "# 임베딩\n",
    "char_embedding = tf.get_variable('char_embedding', [character_size, embedding])\n",
    "embedded = tf.nn.embedding_lookup(char_embedding, x)\n",
    "\n",
    "# 첫 번째 레이어\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.9)\n",
    "#tf.cast(cell, tf.int32)\n",
    "#tf.cast(x, tf.int32)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "output = output[-1]\n",
    "print(\"output shaep\",output.shape)\n",
    "\n",
    "# 두 번째 (아웃풋) 레이어\n",
    "second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "second_layer_bias = bias_variable([output_size])\n",
    "foutput = tf.matmul(output, second_layer_weight) + second_layer_bias\n",
    "#output_sigmoid = tf.sigmoid(output)\n",
    "print(\"output shaep\",foutput.shape)\n",
    "print(\"lables shaep\",y_.shape)\n",
    "# loss와 optimizer\n",
    "#global_step = tf.Variable(0)\n",
    "\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=foutput, labels=y_))\n",
    "ls = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=foutput))\n",
    "#ls = tf.reduce_mean(tf.square(output-y_))\n",
    "\n",
    "#print(loss)\n",
    "\n",
    "#learning_rate= tf.train.exponential_decay(learning_rate, global_step, 10000, 0.75)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(ls)\n",
    "#train_step = tf.train.AdadeltaOptimizer(1.0, 0.95, 1e-6).minimize(cost)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_one_hot(labels):\n",
    "    one_hot = (np.arange(output_size) == labels[:]).astype(np.int32)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_one_hot([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 90.50364303588867 0m 0s\n",
      "epoch: 1  train_loss: 86.01840591430664 0m 0s\n",
      "epoch: 2  train_loss: 69.39417266845703 0m 0s\n",
      "epoch: 3  train_loss: 40.49700927734375 0m 0s\n",
      "epoch: 4  train_loss: 25.245776176452637 0m 0s\n",
      "epoch: 5  train_loss: 15.400376796722412 0m 1s\n",
      "epoch: 6  train_loss: 8.810522556304932 0m 1s\n",
      "epoch: 7  train_loss: 4.99520206451416 0m 1s\n",
      "epoch: 8  train_loss: 3.645711824297905 0m 1s\n",
      "epoch: 9  train_loss: 3.9182249223813415 0m 1s\n",
      "epoch: 10  train_loss: 4.7332282811403275 0m 1s\n",
      "epoch: 11  train_loss: 5.328647017478943 0m 1s\n",
      "epoch: 12  train_loss: 5.48647403717041 0m 2s\n",
      "epoch: 13  train_loss: 5.284876719117165 0m 2s\n",
      "epoch: 14  train_loss: 4.891295067965984 0m 2s\n",
      "epoch: 15  train_loss: 4.466878940351307 0m 2s\n",
      "epoch: 16  train_loss: 4.125562367960811 0m 2s\n",
      "epoch: 17  train_loss: 3.919429298490286 0m 2s\n",
      "epoch: 18  train_loss: 3.843793362379074 0m 2s\n",
      "epoch: 19  train_loss: 3.857130616903305 0m 2s\n",
      "epoch: 20  train_loss: 3.9065564572811127 0m 3s\n",
      "epoch: 21  train_loss: 3.948565751314163 0m 3s\n",
      "epoch: 22  train_loss: 3.9595461189746857 0m 3s\n",
      "epoch: 23  train_loss: 3.9362844824790955 0m 3s\n",
      "epoch: 24  train_loss: 3.8899205029010773 0m 3s\n",
      "epoch: 25  train_loss: 3.83743554353714 0m 3s\n",
      "epoch: 26  train_loss: 3.794029802083969 0m 3s\n",
      "epoch: 27  train_loss: 3.7684358805418015 0m 4s\n",
      "epoch: 28  train_loss: 3.7617359459400177 0m 4s\n",
      "epoch: 29  train_loss: 3.7690876573324203 0m 4s\n",
      "epoch: 30  train_loss: 3.7829663902521133 0m 4s\n",
      "epoch: 31  train_loss: 3.796373777091503 0m 4s\n",
      "epoch: 32  train_loss: 3.8049451261758804 0m 4s\n",
      "epoch: 33  train_loss: 3.8074924871325493 0m 4s\n",
      "epoch: 34  train_loss: 3.805287726223469 0m 5s\n",
      "epoch: 35  train_loss: 3.8007414415478706 0m 5s\n",
      "epoch: 36  train_loss: 3.7961022183299065 0m 5s\n",
      "epoch: 37  train_loss: 3.792681470513344 0m 5s\n",
      "epoch: 38  train_loss: 3.79070608317852 0m 5s\n",
      "epoch: 39  train_loss: 3.7896327823400497 0m 5s\n",
      "epoch: 40  train_loss: 3.7886934131383896 0m 5s\n",
      "epoch: 41  train_loss: 3.787332460284233 0m 6s\n",
      "epoch: 42  train_loss: 3.785437658429146 0m 6s\n",
      "epoch: 43  train_loss: 3.783292070031166 0m 6s\n",
      "epoch: 44  train_loss: 3.781366005539894 0m 6s\n",
      "epoch: 45  train_loss: 3.780085787177086 0m 6s\n",
      "epoch: 46  train_loss: 3.779663071036339 0m 6s\n",
      "epoch: 47  train_loss: 3.7800510227680206 0m 6s\n",
      "epoch: 48  train_loss: 3.780998185276985 0m 7s\n",
      "epoch: 49  train_loss: 3.782185912132263 0m 7s\n",
      "epoch: 50  train_loss: 3.783322349190712 0m 7s\n",
      "epoch: 51  train_loss: 3.784222975373268 0m 7s\n",
      "epoch: 52  train_loss: 3.784816339612007 0m 7s\n",
      "epoch: 53  train_loss: 3.785119667649269 0m 7s\n",
      "epoch: 54  train_loss: 3.7851907312870026 0m 7s\n",
      "epoch: 55  train_loss: 3.7850924879312515 0m 8s\n",
      "epoch: 56  train_loss: 3.784876734018326 0m 8s\n",
      "epoch: 57  train_loss: 3.784583881497383 0m 8s\n",
      "epoch: 58  train_loss: 3.7842446863651276 0m 8s\n",
      "epoch: 59  train_loss: 3.7838949859142303 0m 8s\n",
      "epoch: 60  train_loss: 3.7835788428783417 0m 8s\n",
      "epoch: 61  train_loss: 3.7833337038755417 0m 8s\n",
      "epoch: 62  train_loss: 3.7831884622573853 0m 9s\n",
      "epoch: 63  train_loss: 3.783156782388687 0m 9s\n",
      "epoch: 64  train_loss: 3.7832275331020355 0m 9s\n",
      "epoch: 65  train_loss: 3.783378690481186 0m 9s\n",
      "epoch: 66  train_loss: 3.783576935529709 0m 9s\n",
      "epoch: 67  train_loss: 3.7837853133678436 0m 9s\n",
      "epoch: 68  train_loss: 3.7839780896902084 0m 9s\n",
      "epoch: 69  train_loss: 3.7841345965862274 0m 10s\n",
      "epoch: 70  train_loss: 3.7842439264059067 0m 10s\n",
      "epoch: 71  train_loss: 3.7843057960271835 0m 10s\n",
      "epoch: 72  train_loss: 3.784322053194046 0m 10s\n",
      "epoch: 73  train_loss: 3.7843071669340134 0m 10s\n",
      "epoch: 74  train_loss: 3.784267798066139 0m 10s\n",
      "epoch: 75  train_loss: 3.784219190478325 0m 10s\n",
      "epoch: 76  train_loss: 3.7841692119836807 0m 11s\n",
      "epoch: 77  train_loss: 3.784130483865738 0m 11s\n",
      "epoch: 78  train_loss: 3.7841105610132217 0m 11s\n",
      "epoch: 79  train_loss: 3.7841091454029083 0m 11s\n",
      "epoch: 80  train_loss: 3.7841308265924454 0m 11s\n",
      "epoch: 81  train_loss: 3.784168690443039 0m 11s\n",
      "epoch: 82  train_loss: 3.7842180877923965 0m 11s\n",
      "epoch: 83  train_loss: 3.784272998571396 0m 12s\n",
      "epoch: 84  train_loss: 3.784330576658249 0m 12s\n",
      "epoch: 85  train_loss: 3.7843798249959946 0m 12s\n",
      "epoch: 86  train_loss: 3.784423992037773 0m 12s\n",
      "epoch: 87  train_loss: 3.784456342458725 0m 12s\n",
      "epoch: 88  train_loss: 3.7844794541597366 0m 12s\n",
      "epoch: 89  train_loss: 3.784495919942856 0m 12s\n",
      "epoch: 90  train_loss: 3.784505769610405 0m 13s\n",
      "epoch: 91  train_loss: 3.7845108211040497 0m 13s\n",
      "epoch: 92  train_loss: 3.78451731801033 0m 13s\n",
      "epoch: 93  train_loss: 3.784524366259575 0m 13s\n",
      "epoch: 94  train_loss: 3.7845350950956345 0m 13s\n",
      "epoch: 95  train_loss: 3.784548595547676 0m 13s\n",
      "epoch: 96  train_loss: 3.7845687568187714 0m 13s\n",
      "epoch: 97  train_loss: 3.78459033370018 0m 14s\n",
      "epoch: 98  train_loss: 3.7846133559942245 0m 14s\n",
      "epoch: 99  train_loss: 3.784639820456505 0m 14s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "#batch = 1\n",
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)\n",
    "one_batch_size = dataset_len//batch\n",
    "if dataset_len % batch != 0:\n",
    "    one_batch_size += 1\n",
    "# epoch마다 학습을 수행합니다.\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    avg_l=0.0\n",
    "    for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "        #zero = np.zeros([batch,1])\n",
    "        #zero[0] = \n",
    "        _, l = sess.run([train_step, ls],\n",
    "                           feed_dict={x: data, y_: labels})\n",
    "        #print('Batch : ', i + 1, '/', one_batch_size,', BCE in this minibatch: ', (loss))\n",
    "        avg_l += float(l)\n",
    "        #print(o)\n",
    "    print('epoch:', epoch, ' train_loss:', float(avg_l/one_batch_size),timeSince(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
