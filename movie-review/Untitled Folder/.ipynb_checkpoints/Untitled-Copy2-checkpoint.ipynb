{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import KinQueryDataset, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shaep (?, 200)\n",
      "output shaep (?, 1)\n",
      "lables shaep (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# User options\n",
    "batch = 1\n",
    "epochs = 10\n",
    "\n",
    "embedding = 30\n",
    "strmaxlen = 200\n",
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "# 모델의 specification\n",
    "input_size = embedding*strmaxlen\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "learning_rate = 0.001\n",
    "character_size = 251\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, strmaxlen])\n",
    "y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "# 임베딩\n",
    "char_embedding = tf.get_variable('char_embedding', [character_size, embedding])\n",
    "embedded = tf.nn.embedding_lookup(char_embedding, x)\n",
    "\n",
    "# 첫 번째 레이어\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.9)\n",
    "#tf.cast(cell, tf.int32)\n",
    "#tf.cast(x, tf.int32)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "output = output[-1]\n",
    "print(\"output shaep\",output.shape)\n",
    "\n",
    "# 두 번째 (아웃풋) 레이어\n",
    "second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "second_layer_bias = bias_variable([output_size])\n",
    "foutput = tf.matmul(output, second_layer_weight) + second_layer_bias\n",
    "#output_sigmoid = tf.sigmoid(output)\n",
    "print(\"output shaep\",foutput.shape)\n",
    "print(\"lables shaep\",y_.shape)\n",
    "# loss와 optimizer\n",
    "#global_step = tf.Variable(0)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=foutput, labels=y_))\n",
    "ls = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=foutput))\n",
    "#ls = tf.reduce_mean(tf.square(output-y_))\n",
    "\n",
    "#print(loss)\n",
    "\n",
    "#learning_rate= tf.train.exponential_decay(learning_rate, global_step, 10000, 0.75)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(ls)\n",
    "#train_step = tf.train.AdadeltaOptimizer(1.0, 0.95, 1e-6).minimize(cost)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_one_hot(labels):\n",
    "    one_hot = (np.arange(output_size) == labels[:]).astype(np.int32)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_one_hot([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 0.0 13.132915267369057 0m 9s\n",
      "epoch: 1  train_loss: 0.0 7.361274241738535 0m 19s\n",
      "epoch: 2  train_loss: 0.0 7.695926857803566 0m 29s\n",
      "epoch: 3  train_loss: 0.0 7.508652118679297 0m 39s\n",
      "epoch: 4  train_loss: 0.0 7.338016252726677 0m 48s\n",
      "epoch: 5  train_loss: 0.0 7.280428185985831 0m 58s\n",
      "epoch: 6  train_loss: 0.0 7.251949254158181 1m 8s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "#batch = 1\n",
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)\n",
    "one_batch_size = dataset_len//batch\n",
    "if dataset_len % batch != 0:\n",
    "    one_batch_size += 1\n",
    "# epoch마다 학습을 수행합니다.\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    avg_l=0.0\n",
    "    for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "        #zero = np.zeros([batch,1])\n",
    "        #zero[0] = \n",
    "        _, loss, l,o = sess.run([train_step, cost, ls,foutput],\n",
    "                           feed_dict={x: data, y_: labels})\n",
    "        if math.isnan(loss):\n",
    "            print('Detected NaN')\n",
    "            import pdb; pdb.set_trace()\n",
    "        #print('Batch : ', i + 1, '/', one_batch_size,', BCE in this minibatch: ', (loss))\n",
    "        avg_loss += float(loss)\n",
    "        avg_l += float(l)\n",
    "        #print(o)\n",
    "    print('epoch:', epoch, ' train_loss:', float(avg_loss/one_batch_size),float(avg_l/one_batch_size),timeSince(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ze = np.zeros([batch,2])\n",
    "#ze[0,0] = 1\n",
    "ze = np.zeros([2])\n",
    "ze[0] = 1\n",
    "\n",
    "print(ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "    #t = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    #            logits=ze, labels=one(labels))\n",
    "    for i in range(batch):\n",
    "        print(labels[i])\n",
    "    #print(one(labels), ze, t.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=np.array([[0]])\n",
    "o[0,0]\n",
    "idx = np.argmax(o,axis=0)\n",
    "print(o[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ndarray([,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
