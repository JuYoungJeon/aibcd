{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import KinQueryDataset, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shaep (?, 200)\n",
      "output shaep (?, 1)\n",
      "lables shaep (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# User options\n",
    "batch = 10\n",
    "epochs = 100\n",
    "\n",
    "embedding = 30\n",
    "strmaxlen = 200\n",
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "# 모델의 specification\n",
    "input_size = embedding*strmaxlen\n",
    "output_size = 1\n",
    "hidden_layer_size = 200\n",
    "learning_rate = 0.001\n",
    "character_size = 251\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, strmaxlen])\n",
    "y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "# 임베딩\n",
    "char_embedding = tf.get_variable('char_embedding', [character_size, embedding])\n",
    "embedded = tf.nn.embedding_lookup(char_embedding, x)\n",
    "\n",
    "# 첫 번째 레이어\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.9)\n",
    "#tf.cast(cell, tf.int32)\n",
    "#tf.cast(x, tf.int32)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedded, dtype=tf.float32)\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "output = output[-1]\n",
    "print(\"output shaep\",output.shape)\n",
    "\n",
    "# 두 번째 (아웃풋) 레이어\n",
    "second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "second_layer_bias = bias_variable([output_size])\n",
    "foutput = tf.matmul(output, second_layer_weight) + second_layer_bias\n",
    "#output_sigmoid = tf.sigmoid(output)\n",
    "print(\"output shaep\",foutput.shape)\n",
    "print(\"lables shaep\",y_.shape)\n",
    "# loss와 optimizer\n",
    "#global_step = tf.Variable(0)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=foutput, labels=y_))\n",
    "ls = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=foutput))\n",
    "#ls = tf.reduce_mean(tf.square(output-y_))\n",
    "\n",
    "#print(loss)\n",
    "\n",
    "#learning_rate= tf.train.exponential_decay(learning_rate, global_step, 10000, 0.75)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(ls)\n",
    "#train_step = tf.train.AdadeltaOptimizer(1.0, 0.95, 1e-6).minimize(cost)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.eye(2)[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def label_one_hot(labels):\n",
    "#    one_hot = (np.arange(output_size) == labels[:]).astype(np.int32)\n",
    "#    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_one_hot([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 0.0 49.351018992337316 0m 1s\n",
      "epoch: 1  train_loss: 0.0 8.865284220738845 0m 2s\n",
      "epoch: 2  train_loss: 0.0 6.656276709654114 0m 3s\n",
      "epoch: 3  train_loss: 0.0 6.765658248554576 0m 4s\n",
      "epoch: 4  train_loss: 0.0 7.06935042142868 0m 5s\n",
      "epoch: 5  train_loss: 0.0 7.043845360929316 0m 6s\n",
      "epoch: 6  train_loss: 0.0 6.552712960676714 0m 7s\n",
      "epoch: 7  train_loss: 0.0 6.6933041485873135 0m 8s\n",
      "epoch: 8  train_loss: 0.0 6.661902151324532 0m 9s\n",
      "epoch: 9  train_loss: 0.0 6.757591068744659 0m 10s\n",
      "epoch: 10  train_loss: 0.0 6.653268927877599 0m 11s\n",
      "epoch: 11  train_loss: 0.0 6.702085760506717 0m 12s\n",
      "epoch: 12  train_loss: 0.0 6.543912156061693 0m 13s\n",
      "epoch: 13  train_loss: 0.0 6.695739312605425 0m 14s\n",
      "epoch: 14  train_loss: 0.0 6.382793155583468 0m 15s\n",
      "epoch: 15  train_loss: 0.0 6.655661680481651 0m 16s\n",
      "epoch: 16  train_loss: 0.0 6.748770670457319 0m 17s\n",
      "epoch: 17  train_loss: 0.0 6.814165873961016 0m 18s\n",
      "epoch: 18  train_loss: 0.0 6.722245622764934 0m 19s\n",
      "epoch: 19  train_loss: 0.0 6.503401626240123 0m 20s\n",
      "epoch: 20  train_loss: 0.0 6.763188470493663 0m 21s\n",
      "epoch: 21  train_loss: 0.0 6.603360831737518 0m 22s\n",
      "epoch: 22  train_loss: 0.0 6.914742583578283 0m 23s\n",
      "epoch: 23  train_loss: 0.0 6.695121461694891 0m 24s\n",
      "epoch: 24  train_loss: 0.0 6.726470459591258 0m 25s\n",
      "epoch: 25  train_loss: 0.0 6.88632117618214 0m 26s\n",
      "epoch: 26  train_loss: 0.0 6.8989293141798536 0m 27s\n",
      "epoch: 27  train_loss: 0.0 6.597565883939916 0m 28s\n",
      "epoch: 28  train_loss: 0.0 6.642642904411662 0m 29s\n",
      "epoch: 29  train_loss: 0.0 6.805149018764496 0m 30s\n",
      "epoch: 30  train_loss: 0.0 6.397216022014618 0m 31s\n",
      "epoch: 31  train_loss: 0.0 6.654252653772181 0m 32s\n",
      "epoch: 32  train_loss: 0.0 6.972353761846369 0m 33s\n",
      "epoch: 33  train_loss: 0.0 6.822566119107333 0m 34s\n",
      "epoch: 34  train_loss: 0.0 6.715562815015966 0m 35s\n",
      "epoch: 35  train_loss: 0.0 6.429436033422297 0m 36s\n",
      "epoch: 36  train_loss: 0.0 6.86944737217643 0m 37s\n",
      "epoch: 37  train_loss: 0.0 6.745417475700378 0m 38s\n",
      "epoch: 38  train_loss: 0.0 6.5048557845028965 0m 39s\n",
      "epoch: 39  train_loss: 0.0 6.804057847369801 0m 40s\n",
      "epoch: 40  train_loss: 0.0 6.388149868358266 0m 41s\n",
      "epoch: 41  train_loss: 0.0 6.8208581967787305 0m 42s\n",
      "epoch: 42  train_loss: 0.0 6.734386454929005 0m 43s\n",
      "epoch: 43  train_loss: 0.0 6.752155618234114 0m 44s\n",
      "epoch: 44  train_loss: 0.0 6.567628708752719 0m 45s\n",
      "epoch: 45  train_loss: 0.0 6.5477138270031325 0m 46s\n",
      "epoch: 46  train_loss: 0.0 6.466831077228893 0m 47s\n",
      "epoch: 47  train_loss: 0.0 6.801407348025929 0m 48s\n",
      "epoch: 48  train_loss: 0.0 6.593888326124712 0m 49s\n",
      "epoch: 49  train_loss: 0.0 6.724189330231059 0m 50s\n",
      "epoch: 50  train_loss: 0.0 6.491390932690013 0m 51s\n",
      "epoch: 51  train_loss: 0.0 6.568669806827199 0m 52s\n",
      "epoch: 52  train_loss: 0.0 6.601703903891823 0m 53s\n",
      "epoch: 53  train_loss: 0.0 6.714787131006068 0m 54s\n",
      "epoch: 54  train_loss: 0.0 6.501900526610288 0m 55s\n",
      "epoch: 55  train_loss: 0.0 6.961323564702814 0m 56s\n",
      "epoch: 56  train_loss: 0.0 6.794336849992925 0m 57s\n",
      "epoch: 57  train_loss: 0.0 6.3241870620033955 0m 58s\n",
      "epoch: 58  train_loss: 0.0 6.797288889234716 0m 59s\n",
      "epoch: 59  train_loss: 0.0 6.717651546001434 1m 0s\n",
      "epoch: 60  train_loss: 0.0 6.63646615635265 1m 1s\n",
      "epoch: 61  train_loss: 0.0 6.947671131654219 1m 2s\n",
      "epoch: 62  train_loss: 0.0 6.342529546130788 1m 3s\n",
      "epoch: 63  train_loss: 0.0 6.870756387710571 1m 4s\n",
      "epoch: 64  train_loss: 0.0 6.746843733570793 1m 5s\n",
      "epoch: 65  train_loss: 0.0 6.678303984078494 1m 6s\n",
      "epoch: 66  train_loss: 0.0 6.694522922689265 1m 7s\n",
      "epoch: 67  train_loss: 0.0 6.445923734794963 1m 8s\n",
      "epoch: 68  train_loss: 0.0 6.782166594808752 1m 9s\n",
      "epoch: 69  train_loss: 0.0 6.51985994794152 1m 10s\n",
      "epoch: 70  train_loss: 0.0 6.716248398477381 1m 11s\n",
      "epoch: 71  train_loss: 0.0 6.738548262552782 1m 12s\n",
      "epoch: 72  train_loss: 0.0 6.409684880213304 1m 13s\n",
      "epoch: 73  train_loss: 0.0 6.829462658275258 1m 14s\n",
      "epoch: 74  train_loss: 0.0 6.477472657507116 1m 15s\n",
      "epoch: 75  train_loss: 0.0 6.4305514313957906 1m 16s\n",
      "epoch: 76  train_loss: 0.0 6.701378844001076 1m 17s\n",
      "epoch: 77  train_loss: 0.0 6.530641295693138 1m 18s\n",
      "epoch: 78  train_loss: 0.0 6.98495428128676 1m 19s\n",
      "epoch: 79  train_loss: 0.0 6.629539240490306 1m 20s\n",
      "epoch: 80  train_loss: 0.0 6.590224482796409 1m 21s\n",
      "epoch: 81  train_loss: 0.0 6.633816290985454 1m 22s\n",
      "epoch: 82  train_loss: 0.0 6.726632215759971 1m 23s\n",
      "epoch: 83  train_loss: 0.0 6.75484296950427 1m 24s\n",
      "epoch: 84  train_loss: 0.0 6.727848145094785 1m 25s\n",
      "epoch: 85  train_loss: 0.0 6.753834236751903 1m 26s\n",
      "epoch: 86  train_loss: 0.0 6.739013400944796 1m 27s\n",
      "epoch: 87  train_loss: 0.0 6.511848563497717 1m 28s\n",
      "epoch: 88  train_loss: 0.0 6.72298649224368 1m 29s\n",
      "epoch: 89  train_loss: 0.0 6.443824535066431 1m 30s\n",
      "epoch: 90  train_loss: 0.0 6.836241879246452 1m 31s\n",
      "epoch: 91  train_loss: 0.0 6.508800625801086 1m 32s\n",
      "epoch: 92  train_loss: 0.0 6.779889301820234 1m 33s\n",
      "epoch: 93  train_loss: 0.0 6.647125612605702 1m 34s\n",
      "epoch: 94  train_loss: 0.0 6.661460074511441 1m 35s\n",
      "epoch: 95  train_loss: 0.0 6.613902146166021 1m 36s\n",
      "epoch: 96  train_loss: 0.0 6.856699244542555 1m 37s\n",
      "epoch: 97  train_loss: 0.0 6.429881513118744 1m 39s\n",
      "epoch: 98  train_loss: 0.0 6.337936135855588 1m 40s\n",
      "epoch: 99  train_loss: 0.0 6.329276745969599 1m 41s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "#batch = 1\n",
    "dataset = KinQueryDataset(DATASET_PATH, strmaxlen)\n",
    "dataset_len = len(dataset)\n",
    "one_batch_size = dataset_len//batch\n",
    "if dataset_len % batch != 0:\n",
    "    one_batch_size += 1\n",
    "# epoch마다 학습을 수행합니다.\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    avg_l = 0.0\n",
    "    for i, (data, labels) in enumerate(_batch_loader(dataset, batch)):\n",
    "        #zero = np.zeros([batch,1])\n",
    "        #zero[0] = \n",
    "        _, loss, l = sess.run([train_step, cost, ls],\n",
    "                           feed_dict={x: data, y_: labels})\n",
    "        if math.isnan(loss):\n",
    "            print('Detected NaN')\n",
    "            import pdb; pdb.set_trace()\n",
    "        #print('Batch : ', i + 1, '/', one_batch_size,', BCE in this minibatch: ', (loss))\n",
    "        avg_loss += float(loss)\n",
    "        avg_l += float(l)\n",
    "    print('epoch:', epoch, ' train_loss:', float(avg_loss/one_batch_size),float(avg_l/one_batch_size),timeSince(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
